{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3 - Ingredient Mapping\n",
    "\n",
    "Since different recipes and dataset format recipes and ingredients differently, it is necessary to combine all references to the same ingredient under the same name. For example \"2 eggs\", \"1 lightly beaten egg\", \"eggs\" all refer to the same ingredient, \"egg\". Additionally it is also necessary to account for regional naming differences between ingredients as is seen with \"corriander\" and \"cilantro\".\n",
    "\n",
    "As one final step, it is necessary to match the unified ingredient names to the names used by our compound database *fooDB*.\n",
    "\n",
    "This process can be summarised as:\n",
    "\n",
    "> assorted *individual* ingredient names $\\Rightarrow$ *unified set* of names $\\Rightarrow$ ingredient names in *fooDB*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import json\n",
    "import csv\n",
    "import re #regular expressions\n",
    "from itertools import combinations\n",
    "\n",
    "#fuzzy string matching\n",
    "# >> pip install fuzzywuzzy\n",
    "# >> pip install python-Levenshtein\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importing recipe data\n",
    "Stored format in a `.csv` file is:\n",
    "\n",
    "``recipeName, ingredient1, ingredient2, ...``\n",
    "\n",
    "This is repeated for all recipes in that dataset. Details on how this was achieved can be found in the part 1 submission of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read data from files\n",
    "filepaths = ['data/final/allrecipes_csv1.csv', 'data/final/kaggle_csv1.csv', 'data/final/supplementary_csv1.csv']\n",
    "\n",
    "recipes = []\n",
    "\n",
    "for path in filepaths:\n",
    "    with open(path) as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            recipes.append(row)\n",
    "\n",
    "print(\"Read %d recipes from %d files.\" % (len(recipes), len(filepaths)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a small preprocessing function to try to reduce the number of distinct ingredients in all the recipes. This will:\n",
    "- remove verbs; such as \"chopped\" and \"baked\"\n",
    "- set everything to lowercase letters\n",
    "- perform substitutions; such as \"_\" $\\rightarrow$ \" \" and \"broth\" $\\rightarrow$ \"stock\"\n",
    "- trim/remove stray whitespace characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaceList = {\"_\":\" \", \"broth\": \"stock\"}\n",
    "removeList = [\"baked\", \"smoked\", \"roasted\", \"roast\", \"shredded\", \"sliced\", \"canned\"\n",
    "                , \"chopped\", \"cooked\", \"dried\", \"dry\", \"ground\", \"extra large\", \"large\"\n",
    "                , \"low-fat\", \"light\", \"medium\", \"mixed\", \"minced\", \"nonfat\", \"pickled\"\n",
    "                , \"peeled\", \"pitted\", \"reduced sodium\", \"reduced-fat\", \"unflavored\"\n",
    "                , \"unsweetened\", \"unsalted\", \"beaten\", \"fried\", \"whole\", \"wide\"]\n",
    "\n",
    "def preprocess(ingredient):\n",
    "    ingr = ingredient.lower()\n",
    "    for target in replaceList:\n",
    "        ingr = ingr.replace(target, replaceList[target])\n",
    "    for target in removeList:\n",
    "        ingr = ingr.replace(target, \"\")\n",
    "    ingr = re.sub(' {2,}', ' ', ingr) #remove '  ' and '   ' etc\n",
    "    return ingr.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can collate all the ingredients with some preprocessing into a single combined set (no repeated elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingredientCount = 0\n",
    "ingredients = set()\n",
    "\n",
    "for recipe in recipes:\n",
    "    for ingredient in recipe[1:]:\n",
    "        if ingredient != \"\":\n",
    "            ingredientCount += 1\n",
    "            ingredients.add(preprocess(ingredient))\n",
    "\n",
    "\n",
    "print(\"%d distinct ingredients from a total of %d\" % (len(ingredients), ingredientCount) )\n",
    "\n",
    "ingredients = sorted(ingredients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This appears to have reduced the number of distinct ingredients to only 10% of the original number!\n",
    "\n",
    "The next step is to match and map these to the desired *fooDB* ingredient names!\n",
    "\n",
    "### Matching and Mapping\n",
    "For this task we will try two approaches, simply attempting to match the names to the existing names in the *fooDB* database, and using fuzzy string matching to find a good mapping.\n",
    "\n",
    "This makes use of 2 files of data extracted from the *fooDB* database:\n",
    "- `ingredients.json` which contains a list of ingredient names found in the database\n",
    "- `common-names.json` which contains a mapping from common names/variations of an ingredient to the name in *fooDB*\n",
    "\n",
    "An excerpt of `common-names.json` is below:\n",
    "\n",
    "```json\n",
    "{\n",
    "    ...\n",
    "    \"Dill weed, dried\": \"Dill\",\n",
    "    \"Dill, raw\": \"Dill\",\n",
    "    \"Spices, dill seed\": \"Dill\",\n",
    "    \"Spices, dill weed, dried\": \"Dill\",\n",
    "    \"Dill weed, fresh\": \"Dill\"\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load file data\n",
    "with open(\"data/notebook-3-data/common-names.json\") as file:\n",
    "    referenceNames = json.load(file)\n",
    "\n",
    "with open(\"data/notebook-3-data/ingredients.json\") as file:\n",
    "    referenceIngredients = json.load(file)\n",
    "\n",
    "# initialise collections\n",
    "ingredientNames = list(referenceNames.keys()) # collection of all common names\n",
    "ingredientNames.extend(referenceIngredients) #add regular names to list for direct matching\n",
    "\n",
    "\n",
    "# Create a map from the lowercase version of a common name, back to the original common name entry\n",
    "# all common names are made lower case for better matching and this dictionary is used to restore the original version\n",
    "unlowerMap = {}\n",
    "for index, ingr in enumerate(ingredientNames):\n",
    "    ingredientNames[index] = ingr.lower()\n",
    "    unlowerMap[ingr.lower()] = ingr\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before moving on to the actual matching lets just check that the fuzzy matching is working..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finds the closest common-name matches to the given string (and the confidence values)\n",
    "process.extract(\"chicken wings\", ingredientNames, scorer=fuzz.token_set_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#or just find the single closest match \n",
    "process.extractOne(\"bread crumbs\", ingredientNames, scorer=fuzz.token_set_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the match can then have its case removed and run back through the common-names map to find the foodb name\n",
    "recipeIngredientName = \"bread crumbs\"\n",
    "\n",
    "testMatch = process.extractOne(recipeIngredientName, ingredientNames, scorer=fuzz.token_set_ratio)\n",
    "unlowered = unlowerMap[testMatch[0]]\n",
    "foodbName = referenceNames[unlowered] if unlowered in referenceNames else unlowered\n",
    "\n",
    "print(\"'%s' maps to '%s' in fooDB\" % (recipeIngredientName, foodbName))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this method it is possible to create a map from our ingredient names to the names in *fooDB*. Only matches with a confidence greater than 80% were used, and the rest are put into a separate section with the best guess to be manually verified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duds = {} # incorrect matches or those with insufficient confidence\n",
    "matches = {} # matches with sufficient confidence\n",
    "\n",
    "for index, ingredient in enumerate(ingredients):\n",
    "    if index % 50 == 0 and index != 0:\n",
    "        print(\"done %d items\" % (index) )\n",
    "    match = process.extractOne(ingredient, ingredientNames, scorer=fuzz.token_set_ratio)\n",
    "    if match[1] > 80:\n",
    "        unlowered = unlowerMap[match[0]]\n",
    "        matches[ingredient] = referenceNames[unlowered] if unlowered in referenceNames else unlowered\n",
    "    else:\n",
    "        duds[ingredient] = (referenceNames[unlowered] if unlowered in referenceNames else unlowered, match[0], match[1])\n",
    "\n",
    "print(\"%d out of %d matched successfully (%f percent)\" % (len(matches), len(ingredients), 100*len(matches)/len(ingredients)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see this took a while to process however it was able to \"correctly\" match **87.7%** of the ingredient names.\n",
    "\n",
    "But... unfortuantely not all of these mappings are correct and need to be manually checked. This process is significantly faster than manyally writing mappings for ingredients, however due to this method only finding approximate matches it cannot be relied on completely.\n",
    "\n",
    "While manually checking the mappings, we located approximately an additional 5-6% of items that required fixing. \n",
    "\n",
    "*This means the fuzzy matching process is over 80% accurate!!!* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the results to a file\n",
    "with open(\"data/notebook-3-data/output.json\", 'w') as outfile:\n",
    "    output = {\"successful\": matches, \"failed\": duds}\n",
    "    json.dump(output, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To manually correct mappings in this file, check all entries, fix those that need it and combine all mappings into the top level of the file as shown below:\n",
    "\n",
    "**BEFORE**\n",
    "```json\n",
    "{\n",
    "    \"successful\": {\n",
    "        \"a\": \"apple\",\n",
    "        \"b\": \"banana\",\n",
    "        \"c\": \"lamb\",\n",
    "        \"d\": \"dragon fruit\"\n",
    "    },\n",
    "    \"failed\": {\n",
    "        \"e\": [\"e\", \"rocket salad\", 61]\n",
    "    }\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "**AFTER**\n",
    "```json\n",
    "{\n",
    "    \"a\": \"apple\",\n",
    "    \"b\": \"banana\",\n",
    "    \"c\": \"carrot\",\n",
    "    \"d\": \"dragon fruit\",\n",
    "    \"e\": \"eggplant\"\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the usage of the map, we will load in the completed version of the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/notebook-3-data/ingredientMap (preprocessed_to_foodb).json\") as file:\n",
    "    completedMap = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the mapping, first preprocess the recipe ingredients, then run them through the map...\n",
    "\n",
    "*note that the first element of the array is the recipe title*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recipe as read from original source\n",
    "sampleRecipe = recipes[15]\n",
    "sampleRecipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recipe with ingredients mapped into foodb friendly names\n",
    "mappedRecipe = [sampleRecipe[0]] + [ completedMap[preprocess(ingr)] for ingr in sampleRecipe[1:] ]\n",
    "mappedRecipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mapping the Recipes\n",
    "The final step is to map over all the ingredients of all the recipes.\n",
    "\n",
    "Recall that the first item of the recipe is the title, which cannot be mapped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for recipeIndex, recipe in enumerate(recipes):\n",
    "    for ingrIndex, ingr in enumerate(recipe[1:]):\n",
    "        recipes[recipeIndex][ingrIndex+1] = completedMap[preprocess(ingr)]\n",
    "        \n",
    "#output recipe 15 as example\n",
    "recipes[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"data/after-mapping/ourRecipes.csv\"\n",
    "\n",
    "with open(OUTPUT_PATH, \"w\", newline='') as outFile:\n",
    "    writer = csv.writer(outFile)\n",
    "    for recipe in recipes:\n",
    "        writer.writerow([\"Russian\"]+recipe[1:]) #replace recipe name with cuisine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huzzah! All the recipes have now been mapped!\n",
    "\n",
    "### Combining the Dataset\n",
    "A nice to have feature is a single source of data, which mean that we need to combine all the recipes. This is ideally very simple however it is necessary to remove overlap between the recipes. This particularly relevant between the *supplementary dataset* provided by the paper and the *allrecipes* as the paper's supplementary material drew some of its recipes from an earlier version of the site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reads recipes from a collection of files, removes any duplicate recipes and ingredients and outputs the results\n",
    "# Also creates a map of all occurances of paired ingredients\n",
    "def removeOverlap(fileNameList, outFileName):\n",
    "    # Read Recipes\n",
    "    uniqueRecipes = set()\n",
    "    dupes = dict()\n",
    "    dupeCount = 0\n",
    "    for file_ in fileNameList:\n",
    "        with open(file_) as inFile:\n",
    "            print(\"Parsing File: \" + file_)\n",
    "            reader = csv.reader(inFile)\n",
    "            for row in reader:\n",
    "                cuisine = row[0]\n",
    "                ingredients = set([ingredient.strip() for ingredient in row[1:]]) #remove recipe's duplicate ingredients\n",
    "                entry = (cuisine, tuple(sorted(ingredients)))\n",
    "                #handle duplicates\n",
    "                if entry in uniqueRecipes:\n",
    "                    dupeCount += 1\n",
    "                    if entry in dupes:\n",
    "                        dupes[entry] += 1\n",
    "                    else:\n",
    "                        dupes[entry] = 1\n",
    "                    print(\"Duplicate Entry: \" + str(entry))\n",
    "                    print(\"Occuring \" + str(dupes[entry]) + \" Times\")\n",
    "                uniqueRecipes.add(entry)\n",
    "    print(\"Number of Duplicates is: \" + str(dupeCount))\n",
    "\n",
    "    # Write Recipe List\n",
    "    with open(outFileName + \"Recipes\" + \".csv\", \"w\", newline='') as outFile:\n",
    "        writer = csv.writer(outFile)\n",
    "        for recipe in sorted(uniqueRecipes):\n",
    "            cuisine = recipe[0]\n",
    "            ingredients = recipe[1]\n",
    "            writer.writerow([cuisine] + list(ingredients))\n",
    "\n",
    "    # Count Pairs\n",
    "    pairCount = dict()\n",
    "    for recipe in sorted(uniqueRecipes):\n",
    "        #get all pairs of ingredients\n",
    "        ingredientPairs = combinations(recipe[1], 2)\n",
    "        for pair in ingredientPairs:\n",
    "            if pair in pairCount:\n",
    "                pairCount[pair] += 1\n",
    "            else:\n",
    "                pairCount[pair] = 1\n",
    "\n",
    "    # Write Ingredient Pair Counts\n",
    "    with open(outFileName + \"IngredientPairings\" + \".csv\", \"w\", newline='') as outFile:\n",
    "        writer = csv.writer(outFile)\n",
    "        for pair, count in pairCount.items():\n",
    "            writer.writerow([pair[0], pair[1], count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataPath = \"data/after-mapping/\"\n",
    "outPath = \"data/no-overlap/\"\n",
    "\n",
    "# russian cuisine dataset\n",
    "dataFileNames = [\"ourRecipes.csv\"]\n",
    "mainRecipesFiles = [dataPath + fileName for fileName in dataFileNames]\n",
    "mainOutputFileName = \"noOverlap\"\n",
    "# recipes from other cuisines\n",
    "otherRecipesFiles = [dataPath + \"all_cuisines_csv1.csv\"]\n",
    "otherOutputFileName = \"paperNoOverlap\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map the russian recipes\n",
    "removeOverlap(mainRecipesFiles, outPath + mainOutputFileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map the recipes from other cultures\n",
    "removeOverlap(otherRecipesFiles, outPath + otherOutputFileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*... And thats it!* All the data processing is done and recipes are ready for analysis!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
